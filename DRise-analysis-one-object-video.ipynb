{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\santiago.calderon\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from vision_explanation_methods import DRISE_runner as dr\n",
    "from helpers.pytorch_yolov8_wrapper import PytorchYoloV8Wrapper\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def compute_saliency_maps(results, image_location, model):\n",
    "    # print(image_location)\n",
    "    # print(model)\n",
    "    return dr.get_drise_saliency_map(nummasks=100, imagelocation=image_location, model= PytorchYoloV8Wrapper(model), savename=\"anything\", numclasses=80, max_figures=2, maskres=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def move_image(image, delta_x, delta_y, dimensions):\n",
    "    height, width = image.shape[:2]\n",
    "    new_image = np.zeros((dimensions[0], dimensions[1]))\n",
    "    # Calculate the new position for pasting the image onto the canvas\n",
    "    new_x_start, new_y_start = max(0, delta_x), max(0, delta_y)\n",
    "    new_x_end, new_y_end = min(dimensions[1], width + delta_x), min(\n",
    "        dimensions[0], height + delta_y\n",
    "    )\n",
    "\n",
    "    # Calculate the position to copy from the original image\n",
    "    orig_x_start, orig_y_start = max(0, -delta_x), max(0, -delta_y)\n",
    "    orig_x_end, orig_y_end = min(width, dimensions[1] - delta_x), min(\n",
    "        height, dimensions[0] - delta_y\n",
    "    )\n",
    "\n",
    "    # Copy the image onto the new canvas at the new position\n",
    "    new_image[new_y_start:new_y_end, new_x_start:new_x_end] = image[\n",
    "        orig_y_start:orig_y_end, orig_x_start:orig_x_end\n",
    "    ]\n",
    "\n",
    "    return new_image\n",
    "\n",
    "\n",
    "\n",
    "# image_size = (300, 400)\n",
    "# new_image_size = (600, 800)\n",
    "# color1 = 255\n",
    "# color2 = 0\n",
    "# delta_x = 50\n",
    "# delta_y = -50\n",
    "# test_pattern = np.zeros((image_size[0], image_size[1]), dtype=np.uint8)\n",
    "\n",
    "# for y in range(image_size[0]):\n",
    "#     color = color1 if y % 50 < 25 else color2\n",
    "#     test_pattern[y, :] = color\n",
    "\n",
    "# # Display the test pattern\n",
    "# plt.imshow(test_pattern)\n",
    "# plt.show()\n",
    "\n",
    "# result = move_image(test_pattern, delta_x, delta_y, new_image_size)\n",
    "# plt.imshow(result)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def scale_image(image, scale_x, scale_y):\n",
    "    width = int(image.shape[1] * scale_x)\n",
    "    height = int(image.shape[0] * scale_y)\n",
    "    dim = (width, height)\n",
    "    scaled_image = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n",
    "    return scaled_image\n",
    "\n",
    "# image_size = (400, 300)\n",
    "# color1 = (255, 255, 255)\n",
    "# color2 = (0, 0, 0)\n",
    "\n",
    "# test_pattern = np.zeros((image_size[1], image_size[0], 3), dtype=np.uint8)\n",
    "\n",
    "# for y in range(image_size[1]):\n",
    "#     color = color1 if y % 50 < 25 else color2\n",
    "#     test_pattern[y, :] = color\n",
    "\n",
    "# # Display the test pattern\n",
    "# plt.imshow(test_pattern)\n",
    "# plt.show()\n",
    "\n",
    "# scaled = scale_image(test_pattern, 2, 0.5)\n",
    "# plt.imshow(scaled)\n",
    "# plt.show()\n",
    "\n",
    "# scaled = scale_image(test_pattern, 0.5, 2)\n",
    "# plt.imshow(scaled)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_explanation(img, class_id, exp, first_level, model):\n",
    "    if(first_level == -1):\n",
    "        x = np.flip(np.arange(0, exp.shape[1]))\n",
    "        y = np.arange(0, exp.shape[0])\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "\n",
    "        levels = np.flip(np.unique(exp))\n",
    "        masks = np.empty([exp.shape[0], exp.shape[1], 3])\n",
    "        min_expl = []\n",
    "        # print(class_id)\n",
    "        for level in levels:\n",
    "            pixels = np.where(exp == level)\n",
    "            masks[pixels[0], pixels[1], :] = True\n",
    "            min_expl = np.where(masks, img, 0)\n",
    "            pre = model.predict([min_expl])\n",
    "            if (class_id not in pre[0].boxes.cls.numpy()):\n",
    "                continue\n",
    "            break\n",
    "        first_level = level\n",
    "        print('Finished first explanation')\n",
    "    else:\n",
    "        pixels = np.where(exp >= first_level)\n",
    "        masks = np.empty([exp.shape[0], exp.shape[1], 3])\n",
    "        masks[pixels[0], pixels[1], :] = True\n",
    "        min_expl = np.where(masks, img, 0)\n",
    "    return min_expl, first_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sort import *\n",
    "import cvzone\n",
    "from sort import *\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts.metrics import compute_insertion\n",
    "\n",
    "def calculate_intersection_over_union(square1, square2):\n",
    "    x1 = max(square1[0], square2[0])\n",
    "    y1 = max(square1[1], square2[1])\n",
    "    x2 = min(square1[2], square2[2])\n",
    "    y2 = min(square1[3], square2[3])\n",
    "\n",
    "    area_square1 = (square1[2] - square1[0]) * (square1[3] - square1[1])\n",
    "    area_square2 = (square2[2] - square2[0]) * (square2[3] - square2[1])\n",
    "\n",
    "    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "\n",
    "    union_area = area_square1 + area_square2 - intersection_area\n",
    "\n",
    "    iou = intersection_area / union_area if union_area > 0 else 0\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def get_key_by_value(dictionary, value):\n",
    "    max_iou = 0\n",
    "    best_key = None\n",
    "    for key, square in dictionary.items():\n",
    "        iou = calculate_intersection_over_union(square, value)\n",
    "        if iou > max_iou:\n",
    "            max_iou = iou\n",
    "            best_key = key\n",
    "    return best_key\n",
    "\n",
    "\n",
    "def plot_boxes(results, img, detections, model):\n",
    "    classes = []\n",
    "    confidence = {}\n",
    "    for r in results:\n",
    "        for box in r.boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "\n",
    "            cls = int(box.cls[0])\n",
    "            currentClass = model.model.names[cls]\n",
    "\n",
    "            conf = math.ceil(box.conf[0] * 100) / 100\n",
    "            currentArray = np.array([x1, y1, x2, y2, conf])\n",
    "            detections = np.vstack((detections, currentArray))\n",
    "            classes.append(cls)\n",
    "            confidence[(x1, y1, x2, y2)] = conf\n",
    "            # cvzone.putTextRect(img, f'{str(currentClass).upper()}: {conf}', (x1+10,y1-10), scale=1.5, thickness=2, colorR=(0,0,255),font=cv2.FONT_HERSHEY_PLAIN)\n",
    "            # img = cv2.rectangle(img,(x1,y1),(x2,y2),(0,255,0), thickness=3)\n",
    "    return [detections, classes], img, confidence\n",
    "\n",
    "\n",
    "def track_detect(r, img, tracker, model, first_id, previous_dim, results, exp, first_exp, box_index, image_location, class_name, confidence_dict):\n",
    "    detections, _ = r\n",
    "    resultTracker = tracker.update(detections)\n",
    "    center_changes = {}\n",
    "    counter = 0\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    light_blue = (31, 112, 255)\n",
    "    for i, res in enumerate(resultTracker):\n",
    "        x1, y1, x2,y2, id = res\n",
    "        x1, y1, x2, y2, id = int(x1), int(y1), int(x2), int(y2), int(id)\n",
    "        matching_index_x = 0\n",
    "        if (first_id == id):\n",
    "            x1_old, y1_old, x2_old, y2_old = previous_dim\n",
    "            best_iou = 0\n",
    "            for i, detection in enumerate(detections):\n",
    "                iou = calculate_intersection_over_union((detection[0], detection[1], detection[2], detection[3]), (x1, y1, x2, y2))\n",
    "                if iou > best_iou:\n",
    "                    matching_box = detection\n",
    "                    matching_index_x = i\n",
    "            confidence = confidence_dict[(matching_box[0], matching_box[1], matching_box[2], matching_box[3])]\n",
    "            print(f'x_old: {x1_old}, y_old: {y1_old}, x2_old: {x2_old}, y2_old: {y2_old}')\n",
    "            old_x = abs(x1_old - x2_old)\n",
    "            old_y = abs(y1_old - y2_old)\n",
    "            new_x = abs(x1 - x2)\n",
    "            new_y = abs(y1 - y2)\n",
    "\n",
    "            scale_x = new_x / old_x\n",
    "            scale_y = new_y / old_y\n",
    "            exp_scaled = scale_image(first_exp, scale_x, scale_y)\n",
    "\n",
    "            old_bb_center = np.array([(x1_old + x2_old) / 2, (y1_old + y2_old) / 2])\n",
    "            new_bb_center = np.array([(x1 + x2) / 2, (y1 + y2) / 2])\n",
    "\n",
    "            old_image_center = np.array([width / 2, height / 2])\n",
    "            scaled_image_center = np.array([exp_scaled.shape[1] / 2, exp_scaled.shape[0] / 2])\n",
    "            scaled_bb_center = np.array([(old_bb_center[0] - old_image_center[0]) * scale_x + scaled_image_center[0], (old_bb_center[1] - old_image_center[1]) * scale_y + scaled_image_center[1]])\n",
    "\n",
    "            center_changes = new_bb_center - scaled_bb_center\n",
    "\n",
    "            exp = move_image(\n",
    "                np.transpose(exp_scaled, (0, 1)),\n",
    "                int(center_changes[0]),\n",
    "                int(center_changes[1]),\n",
    "                (height, width),\n",
    "            )\n",
    "            cvzone.putTextRect(\n",
    "                img,\n",
    "                text=f\"{class_name}:{confidence}, id: {first_id}\",\n",
    "                pos=(x1 + 10, y1 - 10),\n",
    "                scale=1.5,\n",
    "                thickness=2,\n",
    "                colorR=light_blue,\n",
    "                font=cv2.FONT_HERSHEY_PLAIN,\n",
    "            )\n",
    "            img = cv2.rectangle(img, (x1, y1), (x2, y2), light_blue, thickness=3)\n",
    "            return (exp, first_exp, previous_dim, first_id, img, (x1, y1, x2, y2), matching_index_x)\n",
    "\n",
    "        if first_id == -1 and (\n",
    "            detections[box_index][0] == x1\n",
    "            and detections[box_index][1] == y1\n",
    "            and detections[box_index][2] == x2\n",
    "            and detections[box_index][3] == y2\n",
    "        ):\n",
    "            print(f\"x: {x1}, y: {y1}, x2: {x2}, y2: {y2}\")\n",
    "            first_id = id\n",
    "            print(f'Class: {class_name}, Confidence: {confidence_dict[(x1,y1,x2,y2)]}')\n",
    "            res = compute_saliency_maps(results, image_location, model)\n",
    "            # print(image_location)\n",
    "            # print(results)\n",
    "            print(\"Finished first saliency map\")\n",
    "            # print(confidence_dict)\n",
    "            cvzone.putTextRect(\n",
    "                img,\n",
    "                text=f\"{class_name}: {confidence_dict[(x1,y1,x2,y2)]}, id: {first_id}\",\n",
    "                pos=(x1 + 10, y1 - 10),\n",
    "                scale=1.5,\n",
    "                thickness=2,\n",
    "                colorR=light_blue,\n",
    "                font=cv2.FONT_HERSHEY_PLAIN,\n",
    "            )\n",
    "            img = cv2.rectangle(img, (x1, y1), (x2, y2), light_blue, thickness=3)\n",
    "            # print(len(res))\n",
    "\n",
    "            return (\n",
    "                res[box_index][\"detection\"][0].numpy(),\n",
    "                res[box_index][\"detection\"][0].numpy(),\n",
    "                (x1, y1, x2, y2),\n",
    "                first_id,\n",
    "                img,\n",
    "                (x1, y1, x2, y2),\n",
    "                box_index\n",
    "            )\n",
    "        counter += 1\n",
    "    return ([], first_exp, previous_dim, first_id, img, previous_dim, box_index)\n",
    "\n",
    "\n",
    "def track_saliency_maps(frame_number, car_number, box_index_first_frame):\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "    tracker = Sort(max_age=20, min_hits=3, iou_threshold=0.3)\n",
    "    first_id = -1\n",
    "    previous_dim = (0, 0, 0, 0)\n",
    "    exp = [0]\n",
    "    first_exp = []\n",
    "    frames = []\n",
    "    auc_results = []\n",
    "    auc_results_2 = []\n",
    "    first_level = -1\n",
    "    class_id = -1\n",
    "    class_name = \"\"\n",
    "    light_blue = (31, 112, 255)\n",
    "    tolerance_counter = 0\n",
    "    max_frame_tolerance = 30 * 5\n",
    "    while True:\n",
    "        image_location = f\"datasets/car/car-{car_number}/img/{str(frame_number).zfill(8)}.jpg\"\n",
    "        if not os.path.exists(image_location):\n",
    "            break\n",
    "        img = cv2.imread(image_location)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = model(img, verbose=False)\n",
    "        if (first_id == -1):\n",
    "            plt.imshow(results[0].plot())\n",
    "            plt.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        detections, frame, confidence_dict = plot_boxes(results, img, np.empty((0,5)), model)\n",
    "        _, classes = detections\n",
    "        if len(results[0].boxes.cls) <= box_index_first_frame:\n",
    "            box_index_first_frame = 0\n",
    "        #print(classes)\n",
    "        if class_name == \"\":\n",
    "            if len(results[0].boxes.cls) == 0:\n",
    "                break\n",
    "            class_name = model.names[classes[box_index_first_frame]]\n",
    "        det, classes = detections\n",
    "        class_dict = {}\n",
    "        for det, cl in zip(det, classes):\n",
    "            class_dict[tuple(det[0:4].astype(np.int32))] = cl\n",
    "        original = img.copy()\n",
    "        #print(detections)\n",
    "        new_exp, first_exp, previous_dim, first_id, img_boxes, current_dim, matching_index = track_detect(\n",
    "                detections,\n",
    "                img,\n",
    "                tracker,\n",
    "                model,\n",
    "                first_id,\n",
    "                previous_dim,\n",
    "                results,\n",
    "                exp,\n",
    "                first_exp,\n",
    "                box_index_first_frame,\n",
    "                image_location,\n",
    "                class_name,\n",
    "                confidence_dict\n",
    "            )\n",
    "        \n",
    "        if abs(previous_dim[0] - previous_dim[2]) == 0 or abs(previous_dim[1] - previous_dim[3]) == 0:\n",
    "            print(\"Skipped\")\n",
    "            break\n",
    "\n",
    "        if len(new_exp) == 0:\n",
    "            tolerance_counter += 1\n",
    "            if tolerance_counter > max_frame_tolerance:\n",
    "                for i in range(max_frame_tolerance):\n",
    "                    frames.pop()\n",
    "                break\n",
    "            frames.append(cv2.hconcat([original, np.zeros_like(original)]))\n",
    "            frame_number += 1\n",
    "            continue\n",
    "        else:\n",
    "            tolerance_counter = 0\n",
    "\n",
    "        exp = new_exp\n",
    "        viridis_frame = plt.cm.viridis(new_exp)\n",
    "        viridis_frame_rgb = viridis_frame[:, :, :3]  # Extract RGB channels\n",
    "\n",
    "        # Blending using OpenCV's addWeighted\n",
    "        alpha = 0.5  # You can adjust the alpha value as needed\n",
    "        dst = cv2.addWeighted(\n",
    "                img_boxes, alpha, (viridis_frame_rgb * 255).astype(np.uint8), 1 - alpha, 0\n",
    "            )\n",
    "        if (class_id == -1):\n",
    "            class_id = results[0].boxes.cls.numpy().astype(\"uint32\")[box_index_first_frame]\n",
    "\n",
    "        min_expl, first_level = compute_explanation(original, class_id, exp, first_level, model)\n",
    "\n",
    "        min_expl = cv2.rectangle(\n",
    "                min_expl,\n",
    "                (current_dim[0], current_dim[1]),\n",
    "                (current_dim[2], current_dim[3]),\n",
    "                light_blue,\n",
    "                thickness=3,\n",
    "            )\n",
    "        print(matching_index, box_index_first_frame)\n",
    "        res = compute_saliency_maps(results, image_location, model)\n",
    "        exp_2 = res[matching_index][\"detection\"][0].numpy()\n",
    "    \n",
    "        viridis_frame = plt.cm.viridis(exp_2)\n",
    "        viridis_frame_rgb = viridis_frame[:, :, :3]  # Extract RGB channels\n",
    "        alpha = 0.5  # You can adjust the alpha value as needed\n",
    "        dst2 = cv2.addWeighted(\n",
    "                img_boxes, alpha, (viridis_frame_rgb * 255).astype(np.uint8), 1 - alpha, 0\n",
    "            )\n",
    "        print(dst2.shape, dst.shape)\n",
    "        final_frame = cv2.hconcat([dst, min_expl, dst2])\n",
    "\n",
    "        frames.append(final_frame)\n",
    "        frame_number += 1\n",
    "        \n",
    "        results = compute_insertion(model = model, saliency_map=exp, image=original, class_index=class_id)\n",
    "        results_2 = compute_insertion(model = model, saliency_map=exp_2, image=original, class_index=class_id)\n",
    "        auc_results.append(results)\n",
    "        auc_results_2.append(results_2)\n",
    "        print(f\"AUC: {results}\")\n",
    "        print(f\"AUC_2: {results_2}\")\n",
    "        \n",
    "    return frames, auc_results, auc_results_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"animation.embed_limit\"] = 2**128\n",
    "\n",
    "plt.ioff()\n",
    "# Function to update each frame\n",
    "def update(frame):\n",
    "    plt.clf()  # Clear the previous plot\n",
    "    plt.imshow(frame, cmap=\"viridis\")  # Show the new frame\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)\n",
    "    plt.gca().set_axis_off()\n",
    "\n",
    "def create_video(update, frames):\n",
    "    ratio = frames[0].shape[0] / frames[0].shape[1]\n",
    "    constant = 8\n",
    "    # Create animation\n",
    "    fig = plt.figure(figsize=(constant, constant * ratio))\n",
    "    ani = animation.FuncAnimation(fig, update, frames=frames, interval=60)\n",
    "    # Display animation as HTML\n",
    "    return HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"savefig.pad_inches\"] = 0\n",
    "import random\n",
    "\n",
    "def save_video(frames, frame_number, car_set_object, box_index):\n",
    "    FPS = 30\n",
    "    out = cv2.VideoWriter(\n",
    "        f\"videos/{frame_number}_{car_set_object}_{box_index}.mp4\",\n",
    "        cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "        FPS,\n",
    "        (frames[0].shape[1], frames[0].shape[0]),\n",
    "    )\n",
    "    for frame in frames:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame)\n",
    "    out.release()\n",
    "\n",
    "def track_saliency_maps_with_video(frame_number, car_set_object, box_index):\n",
    "\n",
    "    print(f\"Frame number: {frame_number}, Car number: {car_set_object}, Explanation index: {box_index}\")\n",
    "\n",
    "    frames, auc_results, aux_results_2 = track_saliency_maps(frame_number=frame_number, car_number=car_set_object, box_index_first_frame=box_index)\n",
    "    \n",
    "    plt.boxplot(auc_results)\n",
    "    plt.show()\n",
    "    plt.boxplot(aux_results_2)\n",
    "    plt.show()\n",
    "    print(f\"Number of frames: {len(frames)}\")\n",
    "    if len(frames) > 0:\n",
    "        display(create_video(update, frames))\n",
    "        save_video(frames, frame_number, car_set_object, box_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame number: 1468, Car number: 10, Explanation index: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrack_saliency_maps_with_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1468\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# for j in range(1, 100):\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     frame_number = random.randint(1, 1500)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#     car_number = random.randint(11, 20)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#     box_index = random.randint(0, 3)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#     track_saliency_maps_with_video(frame_number, car_number, box_index)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m, in \u001b[0;36mtrack_saliency_maps_with_video\u001b[1;34m(frame_number, car_set_object, box_index)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrack_saliency_maps_with_video\u001b[39m(frame_number, car_set_object, box_index):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrame number: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Car number: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcar_set_object\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Explanation index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbox_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m     frames, auc_results, aux_results_2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrack_saliency_maps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcar_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcar_set_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_index_first_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbox_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     plt\u001b[38;5;241m.\u001b[39mboxplot(auc_results)\n\u001b[0;32m     25\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[1;32mIn[6], line 182\u001b[0m, in \u001b[0;36mtrack_saliency_maps\u001b[1;34m(frame_number, car_number, box_index_first_frame)\u001b[0m\n\u001b[0;32m    180\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_location)\n\u001b[0;32m    181\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m--> 182\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (first_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    184\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot())\n",
      "File \u001b[1;32mc:\\Users\\santiago.calderon\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\ultralytics\\engine\\model.py:177\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    156\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    157\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    159\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m    An alias for the predict method, enabling the model instance to be callable.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m        (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\santiago.calderon\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\ultralytics\\engine\\model.py:446\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictor\u001b[39m\u001b[38;5;124m\"\u001b[39m)(overrides\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_cli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, args)\n",
      "File \u001b[1;32mc:\\Users\\santiago.calderon\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:299\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[1;34m(self, model, verbose)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    296\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize YOLO model with given parameters and set it to evaluation mode.\"\"\"\u001b[39;00m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m AutoBackend(\n\u001b[0;32m    298\u001b[0m         weights\u001b[38;5;241m=\u001b[39mmodel \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m--> 299\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    300\u001b[0m         dnn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdnn,\n\u001b[0;32m    301\u001b[0m         data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata,\n\u001b[0;32m    302\u001b[0m         fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf,\n\u001b[0;32m    303\u001b[0m         batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[0;32m    304\u001b[0m         fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    305\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    306\u001b[0m     )\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\santiago.calderon\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\ultralytics\\utils\\torch_utils.py:164\u001b[0m, in \u001b[0;36mselect_device\u001b[1;34m(device, batch, newline, verbose)\u001b[0m\n\u001b[0;32m    162\u001b[0m     arg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# revert to CPU\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m     s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mget_cpu_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    165\u001b[0m     arg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32mc:\\Users\\santiago.calderon\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\ultralytics\\utils\\torch_utils.py:73\u001b[0m, in \u001b[0;36mget_cpu_info\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcpuinfo\u001b[39;00m  \u001b[38;5;66;03m# pip install py-cpuinfo\u001b[39;00m\n\u001b[0;32m     72\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrand_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhardware_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124march_string_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# info keys sorted by preference (not all keys always available)\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43mcpuinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cpu_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# info dict\u001b[39;00m\n\u001b[0;32m     74\u001b[0m string \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39mget(k[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m k[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m info \u001b[38;5;28;01melse\u001b[39;00m k[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m k[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m info \u001b[38;5;28;01melse\u001b[39;00m k[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m string\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(R)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@ \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\santiago.calderon\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\cpuinfo\\cpuinfo.py:2759\u001b[0m, in \u001b[0;36mget_cpu_info\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2752\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m   2753\u001b[0m \u001b[38;5;124;03mReturns the CPU info by using the best sources of information for your OS.\u001b[39;00m\n\u001b[0;32m   2754\u001b[0m \u001b[38;5;124;03mReturns the result in a dict\u001b[39;00m\n\u001b[0;32m   2755\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m   2757\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m-> 2759\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mget_cpu_info_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2761\u001b[0m \u001b[38;5;66;03m# Convert JSON to Python with non unicode strings\u001b[39;00m\n\u001b[0;32m   2762\u001b[0m output \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(output, object_hook \u001b[38;5;241m=\u001b[39m _utf_to_str)\n",
      "File \u001b[1;32mc:\\Users\\santiago.calderon\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\cpuinfo\\cpuinfo.py:2742\u001b[0m, in \u001b[0;36mget_cpu_info_json\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2740\u001b[0m command \u001b[38;5;241m=\u001b[39m [sys\u001b[38;5;241m.\u001b[39mexecutable, \u001b[38;5;18m__file__\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--json\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m   2741\u001b[0m p1 \u001b[38;5;241m=\u001b[39m Popen(command, stdout\u001b[38;5;241m=\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39mPIPE, stdin\u001b[38;5;241m=\u001b[39mPIPE)\n\u001b[1;32m-> 2742\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mp1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   2744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p1\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2745\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\santiago.calderon\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\santiago.calderon\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\subprocess.py:1628\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;66;03m# Wait for the reader threads, or time out.  If we time out, the\u001b[39;00m\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;66;03m# threads remain reading and the fds left open in case the user\u001b[39;00m\n\u001b[0;32m   1626\u001b[0m \u001b[38;5;66;03m# calls communicate again.\u001b[39;00m\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1628\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remaining_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1629\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m   1630\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, orig_timeout)\n",
      "File \u001b[1;32mc:\\Users\\santiago.calderon\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\threading.py:1119\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1121\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\santiago.calderon\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\threading.py:1139\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1140\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "track_saliency_maps_with_video(1468, 10, 1)\n",
    "\n",
    "# for j in range(1, 100):\n",
    "#     frame_number = random.randint(1, 1500)\n",
    "#     car_number = random.randint(11, 20)\n",
    "#     while car_number == 7 or car_number == 5 or car_number == 4:\n",
    "#         car_number = random.randint(1, 10)\n",
    "#     box_index = random.randint(0, 3)\n",
    "#     track_saliency_maps_with_video(frame_number, car_number, box_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
